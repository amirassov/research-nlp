# RESEARCH-NLP
Данный репозиторий является отчетом о моей научной деятельности.

## 2018/10/01:
Прочитал следующие статьи:

**[2017] Attention Is All You Need** [[paper]](https://arxiv.org/abs/1706.03762), [[code]](https://github.com/jadore801120/attention-is-all-you-need-pytorch). 

Предлагается новая модель `Transformer` для задачи `seq2seq`, которая не использует ни `RNN`, ни `CNN`, а только механизмы внимания. 

**[2017] Regularizing and Optimizing LSTM Language Models** [[paper]](https://arxiv.org/abs/1708.02182), [[code]](https://github.com/salesforce/awd-lstm-lm). 

В статье описываются стратегии регуляризации и оптимизации для задачи 
языковой модели: 
* `DropConnect` для скрытых слоев `LSTM` в качестве метода регуляризации. 
* Последовательности переменной длины для `backpropagation`.
* `Variational dropout` для входных и выходных слоев `LSTM`.
* `Embedding dropout`.
* `Weight tying`. Связывание весов эмбединга энкодера и декодера.

Авторы показывают SOTA результаты на двух датасетах в задаче языковой модели.

**[2018] Universal Language Model Fine-tuning for Text Classification** [[paper]](https://arxiv.org/abs/1801.06146), [[code]](https://github.com/fastai/fastai/tree/master/fastai/text). 

В статье предлагается применить `transfer learning` для задачи классификации текстов:
* В качестве основной модели используется архитектура из предыдущей статьи.
* Модель предобучается на задаче языковой модели на датасете `Wikitext-103` из 103 млн. слов.
* Модель дообучается на данных целевой задачи:
    * В качестве алгоритма оптимизации используется `SGD`.
    * Различные параметры `learning rate` для различных слоев сети 
    (`discriminative fine-tuning`): чем глубже слой, тем больше `learning rate`.
    * `learning rate` обновляется по принципу `1cycle`.
* Полученную модель дообучаем уже на целевой задаче классификации:
    * `Concat pooling`: объединяем скрытое состояние на последнем шаге с 
    максимальным и средним представлением скрытых состояний документа. 
    * Постепенно размораживаем слои начиная с последнего слоя, 
    поскольку она содержит наименее общие знания.
    * Двунаправленный `LSTM`. 

На пяти датасетах авторы достигают существенного улучшения.
